<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Reinforcement Learning Study Guide</title>
    <!-- Chosen Palette: Calm Neutrals -->
    <!-- Application Structure Plan: A single-page application with a top navigation bar for thematic sections (Foundations, Core Algorithms, Advanced Techniques). Each section contains introductory text, interactive modules (like an Algorithm Comparator and visual concept explorers), and a sample question simulator. This thematic, interactive structure is chosen over a linear one to promote non-linear exploration, facilitate direct comparison of concepts, and encourage active learning through quizzes, making it more effective for exam preparation than a static document. -->
    <!-- Visualization & Content Choices: 
        - Stationary vs. Non-Stationary: Interactive line chart (Chart.js) with a slider to demonstrate reward decay rates, making abstract formulas visual. Goal: Compare.
        - Epsilon-Greedy vs. Softmax: An "Algorithm Comparator" with side-by-side text and a dynamic bar chart (Chart.js) to show action probability distributions, clarifying their different exploration behaviors. Goal: Compare/Organize.
        - SARSA vs. Q-Learning: A clickable CliffWorld grid diagram (HTML/CSS) to visually highlight the resulting paths, directly illustrating the on-policy vs. off-policy difference in a risk-sensitive environment. Goal: Compare/Inform.
        - Hierarchical Optimality: A simple bar chart (Chart.js) with hover-to-reveal definitions to provide a quick visual anchor for the R <= H <= F relationship. Goal: Inform.
        - Sample Questions: An interactive quiz module for active recall, transforming passive Q&A into an engaging test of knowledge. Goal: Organize/Inform.
    -->
    <!-- CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #fdfdfd;
            color: #1a202c;
        }
        .nav-link {
            transition: color 0.3s ease, border-color 0.3s ease;
        }
        .nav-link.active {
            color: #3b82f6;
            border-bottom-color: #3b82f6;
        }
        .concept-card {
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }
        .concept-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
        }
        .quiz-option {
            transition: background-color 0.3s ease, transform 0.2s ease;
        }
        .quiz-option:hover {
            transform: translateY(-2px);
        }
        .cliff-grid-cell {
            width: 100%;
            padding-bottom: 100%;
            position: relative;
        }
        .cliff-grid-cell-content {
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
        }
    </style>
</head>
<body class="antialiased">

    <div id="app-container" class="min-h-screen">
        <header class="bg-white/80 backdrop-blur-md shadow-sm sticky top-0 z-50">
            <nav class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
                <div class="flex items-center justify-between h-16">
                    <div class="flex items-center">
                        <span class="font-bold text-xl text-gray-800">RL Study Guide</span>
                    </div>
                    <div class="hidden md:block">
                        <div class="ml-10 flex items-baseline space-x-4">
                            <a href="#welcome" class="nav-link text-gray-600 hover:text-blue-500 px-3 py-2 rounded-md text-sm font-medium border-b-2 border-transparent">Welcome</a>
                            <a href="#foundations" class="nav-link text-gray-600 hover:text-blue-500 px-3 py-2 rounded-md text-sm font-medium border-b-2 border-transparent">Foundations</a>
                            <a href="#algorithms" class="nav-link text-gray-600 hover:text-blue-500 px-3 py-2 rounded-md text-sm font-medium border-b-2 border-transparent">Core Algorithms</a>
                            <a href="#advanced" class="nav-link text-gray-600 hover:text-blue-500 px-3 py-2 rounded-md text-sm font-medium border-b-2 border-transparent">Advanced Topics</a>
                        </div>
                    </div>
                </div>
            </nav>
        </header>

        <main>
            <section id="welcome" class="app-section max-w-7xl mx-auto py-12 px-4 sm:px-6 lg:px-8">
                <div class="text-center">
                    <h1 class="text-4xl font-extrabold text-gray-900 sm:text-5xl md:text-6xl">
                        Interactive Reinforcement Learning Study Guide
                    </h1>
                    <p class="mt-3 max-w-md mx-auto text-base text-gray-500 sm:text-lg md:mt-5 md:text-xl md:max-w-3xl">
                        An interactive application designed to help you master the core concepts of Reinforcement Learning. Explore topics, compare algorithms, and test your knowledge with interactive quizzes based on your course assignments.
                    </p>
                </div>
                <div class="mt-12 bg-blue-50 border-l-4 border-blue-400 p-4 rounded-r-lg">
                    <div class="flex">
                        <div class="flex-shrink-0">
                            <svg class="h-5 w-5 text-blue-400" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true">
                                <path fill-rule="evenodd" d="M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-7-4a1 1 0 11-2 0 1 1 0 012 0zM9 9a1 1 0 000 2v3a1 1 0 001 1h1a1 1 0 100-2v-3a1 1 0 00-1-1H9z" clip-rule="evenodd" />
                            </svg>
                        </div>
                        <div class="ml-3">
                            <p class="text-sm text-blue-700">
                                Use the navigation bar at the top to jump between sections. Within each section, you'll find explanations, interactive visualizations, and a quiz to solidify your understanding.
                            </p>
                        </div>
                    </div>
                </div>
            </section>
            
            <section id="foundations" class="app-section hidden bg-gray-50 py-16 sm:py-20">
                <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
                    <div class="text-center">
                        <h2 class="text-3xl font-extrabold text-gray-900 sm:text-4xl">Foundations: Bandits & MDPs</h2>
                        <p class="mt-4 text-lg text-gray-500">Understanding the core dilemma of exploration vs. exploitation and the mathematical framework for decision-making.</p>
                    </div>

                    <div class="mt-12 grid gap-8 md:grid-cols-2 lg:grid-cols-2">
                        <div class="concept-card bg-white p-6 rounded-lg shadow-lg">
                            <h3 class="text-xl font-semibold mb-3">Stationary vs. Non-Stationary Bandits</h3>
                            <p class="text-gray-600 mb-4">A key distinction in bandit problems is whether the reward probabilities are fixed (stationary) or change over time (non-stationary). This choice dictates the optimal Q-value update strategy.</p>
                             <div class="chart-container relative h-64 w-full max-w-xl mx-auto">
                                <canvas id="banditDecayChart"></canvas>
                            </div>
                            <div class="mt-4">
                                <label for="alpha-slider" class="block text-sm font-medium text-gray-700">Learning Rate (α) for Non-Stationary: <span id="alpha-value">0.1</span></label>
                                <input id="alpha-slider" type="range" min="0.01" max="0.5" step="0.01" value="0.1" class="w-full h-2 bg-gray-200 rounded-lg appearance-none cursor-pointer">
                            </div>
                            <p class="text-sm text-gray-500 mt-2">Stationary problems use a simple average (hyperbolic decay), giving equal weight to all past rewards. Non-stationary problems use a constant learning rate α (exponential decay) to give more weight to recent rewards, allowing the agent to adapt to changes.</p>
                        </div>

                        <div class="concept-card bg-white p-6 rounded-lg shadow-lg">
                             <h3 class="text-xl font-semibold mb-3">The Markov Property</h3>
                            <p class="text-gray-600 mb-4">The future is independent of the past given the present. This assumption is the bedrock of MDPs, stating that the current state $s_t$ captures all relevant information from history to predict the next state $s_{t+1}$ and reward $r_{t+1}$.</p>
                            <div class="bg-gray-100 p-4 rounded-lg">
                                <p class="text-center font-mono text-sm sm:text-base text-gray-800 break-words">$Pr(s_{t+1}, r_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ...) = Pr(s_{t+1}, r_{t+1} | s_t, a_t)$</p>
                            </div>
                            <p class="text-sm text-gray-500 mt-4">This property makes RL problems tractable. Without it, the "state" would need to include the entire history, leading to an unmanageable explosion of complexity. It allows us to build value functions based only on the current state.</p>
                        </div>
                    </div>
                    
                    <div id="foundations-quiz" class="mt-16"></div>
                </div>
            </section>

            <section id="algorithms" class="app-section hidden py-16 sm:py-20">
                <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
                     <div class="text-center">
                        <h2 class="text-3xl font-extrabold text-gray-900 sm:text-4xl">Core Algorithms</h2>
                        <p class="mt-4 text-lg text-gray-500">Explore and compare the fundamental algorithms for solving Reinforcement Learning problems.</p>
                    </div>

                    <div class="mt-12 concept-card bg-white p-6 rounded-lg shadow-lg">
                        <h3 class="text-xl font-semibold mb-3">Algorithm Comparator</h3>
                        <p class="text-gray-600 mb-4">Select two concepts to see a side-by-side comparison of their properties and behaviors.</p>
                        <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-6">
                            <select id="comparator-1" class="block w-full pl-3 pr-10 py-2 text-base border-gray-300 focus:outline-none focus:ring-blue-500 focus:border-blue-500 sm:text-sm rounded-md">
                                <option>Epsilon-Greedy</option>
                                <option>Softmax</option>
                                <option>SARSA</option>
                                <option>Q-Learning</option>
                            </select>
                            <select id="comparator-2" class="block w-full pl-3 pr-10 py-2 text-base border-gray-300 focus:outline-none focus:ring-blue-500 focus:border-blue-500 sm:text-sm rounded-md">
                                <option>Softmax</option>
                                <option>Epsilon-Greedy</option>
                                <option>Q-Learning</option>
                                <option>SARSA</option>
                            </select>
                        </div>
                        <div id="comparator-results" class="grid grid-cols-1 md:grid-cols-2 gap-6">
                            <div id="comparator-col-1" class="border border-gray-200 p-4 rounded-lg"></div>
                            <div id="comparator-col-2" class="border border-gray-200 p-4 rounded-lg"></div>
                        </div>
                    </div>

                    <div class="mt-12 concept-card bg-white p-6 rounded-lg shadow-lg">
                         <h3 class="text-xl font-semibold mb-3">Case Study: CliffWorld</h3>
                         <p class="text-gray-600 mb-4">This classic example highlights the crucial difference between on-policy (SARSA) and off-policy (Q-Learning) algorithms in a risk-sensitive environment. The agent starts at 'S' and wants to reach 'G'. The cliff incurs a large negative reward.</p>
                         <div class="flex flex-col md:flex-row gap-8 items-center">
                            <div class="w-full md:w-1/2">
                                <div id="cliff-grid" class="grid grid-cols-12 gap-1"></div>
                            </div>
                            <div class="w-full md:w-1/2">
                                <div class="flex space-x-4 mb-4">
                                    <button id="sarsa-btn" class="bg-blue-500 text-white font-bold py-2 px-4 rounded-lg hover:bg-blue-600 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500">Show SARSA Path</button>
                                    <button id="qlearning-btn" class="bg-red-500 text-white font-bold py-2 px-4 rounded-lg hover:bg-red-600 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-red-500">Show Q-Learning Path</button>
                                </div>
                                <p id="cliff-explanation" class="text-gray-600">Select an algorithm to see its chosen path and an explanation.</p>
                            </div>
                         </div>
                    </div>
                     <div id="algorithms-quiz" class="mt-16"></div>
                </div>
            </section>
            
            <section id="advanced" class="app-section hidden bg-gray-50 py-16 sm:py-20">
                <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
                    <div class="text-center">
                        <h2 class="text-3xl font-extrabold text-gray-900 sm:text-4xl">Advanced Topics</h2>
                        <p class="mt-4 text-lg text-gray-500">Tackling complexity with function approximation, deep learning, and hierarchical approaches.</p>
                    </div>

                    <div class="mt-12 grid gap-8 md:grid-cols-2">
                        <div class="concept-card bg-white p-6 rounded-lg shadow-lg">
                            <h3 class="text-xl font-semibold mb-3">Deep Q-Networks (DQN)</h3>
                            <p class="text-gray-600 mb-4">DQN revolutionized RL by using deep neural networks to approximate Q-values, enabling agents to learn from high-dimensional inputs like pixels. Two key innovations ensure stability:</p>
                            <ul class="list-disc list-inside space-y-2 text-gray-600">
                                <li><strong>Experience Replay:</strong> A buffer of past experiences is stored and sampled randomly. This breaks correlations between sequential samples and improves data efficiency.</li>
                                <li><strong>Target Network:</strong> A separate, periodically updated network provides stable Q-value targets, preventing the "moving target" problem and stabilizing training.</li>
                            </ul>
                            <div class="mt-4 bg-yellow-50 border-l-4 border-yellow-400 p-4 rounded-r-lg">
                                <p class="text-sm text-yellow-700">Even with these techniques, DQN still requires an explicit exploration strategy (like $\epsilon$-greedy) to ensure it discovers the environment effectively.</p>
                            </div>
                        </div>
                        <div class="concept-card bg-white p-6 rounded-lg shadow-lg">
                            <h3 class="text-xl font-semibold mb-3">Hierarchical Reinforcement Learning (HRL)</h3>
                             <p class="text-gray-600 mb-4">HRL structures complex problems by creating a hierarchy of policies. This offers temporal abstraction (high-level actions that take multiple steps) and state abstraction, making learning more efficient and scalable.</p>
                            <div class="chart-container relative h-64 w-full max-w-xl mx-auto">
                                <canvas id="hrlOptimalityChart"></canvas>
                            </div>
                             <p class="text-sm text-gray-500 mt-2">HRL policies are often evaluated against different optimality criteria. Hover over the bars to learn more. The goal is to trade a small amount of optimality for a large gain in tractability.</p>
                        </div>
                    </div>
                    <div id="advanced-quiz" class="mt-16"></div>
                </div>
            </section>

        </main>
    </div>

    <script>
        const appData = {
            comparisons: {
                'Epsilon-Greedy': {
                    title: 'Epsilon-Greedy',
                    description: 'A simple exploration strategy that balances exploiting the best-known action and exploring random actions.',
                    properties: [
                        'With probability 1-ε, choose the greedy action.',
                        'With probability ε, choose a random action.',
                        'Exploration is uniform and "blind" to action values.',
                        'Simple to implement and often effective.'
                    ],
                    cons: 'Can waste time exploring obviously bad actions.'
                },
                'Softmax': {
                    title: 'Softmax (Boltzmann) Exploration',
                    description: 'A more sophisticated strategy where action probabilities are proportional to their value estimates.',
                    properties: [
                        'Action probability is $e^{Q(s,a)/\\tau} / \\sum e^{Q(s,b)/\\tau}$',
                        'The temperature parameter τ controls greediness.',
                        'Prefers exploring more promising actions.',
                        'Avoids exploring clearly inferior actions.'
                    ],
                    cons: 'Requires careful tuning of the temperature parameter.'
                },
                'SARSA': {
                    title: 'SARSA (On-Policy TD Control)',
                    description: 'Learns the value of the policy it is currently following, including its own exploratory moves.',
                    properties: [
                        'Update rule: $Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma Q(s\',a\') - Q(s,a)]$.',
                        'Uses the action $a\'$ actually taken in the next state.',
                        'Tends to be more cautious and risk-averse.',
                        'Finds a "safer" path in risk-sensitive tasks.'
                    ],
                    cons: 'The learned policy can be suboptimal if exploration is high.'
                },
                'Q-Learning': {
                    title: 'Q-Learning (Off-Policy TD Control)',
                    description: 'Learns the optimal value function directly, regardless of the policy being followed for exploration.',
                    properties: [
                        'Update rule: $Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max_{a\'} Q(s\',a\') - Q(s,a)]$.',
                        'Uses the greedy action in the next state for the update.',
                        'Can learn the optimal policy even while behaving randomly.',
                        'Finds the true optimal path.'
                    ],
                    cons: 'Can be less stable than on-policy methods.'
                }
            },
            cliffWorld: {
                sarsa: "SARSA is on-policy, so it learns the value of its own exploratory policy. When it occasionally explores and falls off the cliff (receiving a reward of -100), this large negative reward is incorporated into the Q-values of the path near the cliff. As a result, it learns that the path along the cliff edge is risky and converges to the longer, but safer, blue path to maximize reward under its own behavior.",
                qlearning: "Q-Learning is off-policy. Its update uses the maximum Q-value for the next state, effectively learning the value of the optimal (greedy) policy, not the exploratory one. Even if it falls off the cliff during exploration, it learns that the *optimal* action from a state near the cliff is to move away from it. Therefore, it correctly identifies the shortest (red) path as the optimal one."
            },
            quizzes: {
                foundations: [
                    {
                        question: "In a non-stationary multi-armed bandit problem, why is a constant learning rate (α) preferred over a simple average for Q-value updates?",
                        options: [
                            "It guarantees faster convergence to the true values.",
                            "It gives more weight to recent rewards, allowing the agent to adapt to changing reward distributions.",
                            "It simplifies the computation and requires less memory.",
                            "It ensures the agent explores more diverse actions."
                        ],
                        correct: 1,
                        explanation: "In non-stationary problems, reward distributions change over time. A constant learning rate α causes an exponential decay of past rewards' influence, effectively 'forgetting' old, potentially irrelevant information and adapting to the most recent data. A simple average gives equal weight to all past rewards, making it very slow to adapt to changes."
                    },
                    {
                        question: "Which of the following is NOT a valid norm function for a vector?",
                        options: [
                            "Maximum absolute value of any element.",
                            "Sum of the absolute values of the elements.",
                            "Minimum absolute value of any element.",
                            "Square root of the sum of squared elements."
                        ],
                        correct: 2,
                        explanation: "A valid norm must be zero if and only if the vector is the zero vector. The minimum absolute value can be zero for a non-zero vector (e.g., [0, 5]), so it is not a valid norm."
                    }
                ],
                algorithms: [
                     {
                        question: "What is the key difference between SARSA and Q-Learning's update rule?",
                        options: [
                            "SARSA uses a smaller learning rate.",
                            "Q-Learning uses a larger discount factor.",
                            "SARSA is model-based, while Q-Learning is model-free.",
                            "SARSA uses the next action actually taken by the policy for its update, while Q-Learning uses the maximum possible Q-value of the next state."
                        ],
                        correct: 3,
                        explanation: "This is the fundamental on-policy vs. off-policy distinction. SARSA's update depends on the quintuple (S, A, R, S', A'), making it on-policy. Q-Learning's update depends on (S, A, R, S') and then finds the max Q-value, making it off-policy as it learns about the greedy policy, not the one it's following."
                    },
                    {
                        question: "Which of these methods perform updates only at the end of a full episode?",
                        options: [
                            "Dynamic Programming",
                            "Temporal Difference (TD) Learning",
                            "Monte Carlo (MC) Methods",
                            "All of the above"
                        ],
                        correct: 2,
                        explanation: "Monte Carlo methods learn from complete trajectories and can only calculate the full return Gt once an episode terminates. DP and TD methods both use bootstrapping, allowing them to update value estimates before the end of an episode."
                    }
                ],
                advanced: [
                    {
                        question: "What is the primary purpose of the 'target network' in Deep Q-Networks (DQN)?",
                        options: [
                            "To increase the speed of training.",
                            "To provide a stable, non-moving target for Q-value updates, improving training stability.",
                            "To handle continuous action spaces.",
                            "To store past experiences for replay."
                        ],
                        correct: 1,
                        explanation: "The target network's weights are frozen for a period of time, providing a stable target for the main network's updates. This prevents the 'moving target' problem where the update targets change at every step, which can lead to oscillations and divergence."
                    },
                    {
                        question: "In Hierarchical RL, what is the expected relationship between the rewards of a Flat Optimal (F), Hierarchically Optimal (H), and Recursively Optimal (R) policy?",
                        options: [
                            "F <= H <= R",
                            "R <= H <= F",
                            "H <= R <= F",
                            "F = H = R"
                        ],
                        correct: 1,
                        explanation: "The Flat Optimal policy is unconstrained and finds the true global optimum. The Hierarchically Optimal policy is constrained by the predefined hierarchy, so its performance is less than or equal to the flat one. The Recursively Optimal policy is greedy at each sub-problem level, which may not be globally optimal, so its performance is less than or equal to the hierarchically optimal one."
                    }
                ]
            }
        };

        document.addEventListener('DOMContentLoaded', () => {
            const sections = document.querySelectorAll('.app-section');
            const navLinks = document.querySelectorAll('.nav-link');

            function updateActiveLink() {
                let current = '';
                sections.forEach(section => {
                    const sectionTop = section.offsetTop;
                    if (pageYOffset >= sectionTop - 100) {
                        current = section.getAttribute('id');
                    }
                });
                
                if(!current && window.location.hash) {
                    current = window.location.hash.substring(1);
                } else if (!current) {
                    current = 'welcome';
                }

                navLinks.forEach(link => {
                    link.classList.remove('active', 'text-blue-500');
                    if (link.getAttribute('href') === `#${current}`) {
                        link.classList.add('active', 'text-blue-500');
                    }
                });
            }
            
            function navigateToSection(hash) {
                sections.forEach(section => section.classList.add('hidden'));
                const targetSection = document.querySelector(hash);
                if (targetSection) {
                    targetSection.classList.remove('hidden');
                }
                updateActiveLink();
            }


            navLinks.forEach(link => {
                link.addEventListener('click', (e) => {
                    e.preventDefault();
                    const hash = e.target.getAttribute('href');
                    history.pushState(null, null, hash);
                    navigateToSection(hash);
                });
            });

            window.addEventListener('popstate', () => {
                navigateToSection(window.location.hash || '#welcome');
            });
            
            window.addEventListener('scroll', updateActiveLink);

            // Initial load
            const initialHash = window.location.hash || '#welcome';
            navigateToSection(initialHash);
            if (initialHash === '#welcome') {
                document.querySelector('#welcome').classList.remove('hidden');
            }


            // Initialize interactives
            initBanditChart();
            initComparator();
            initCliffWorld();
            initHRLChart();
            initQuizzes();
        });

        let banditDecayChart;
        function initBanditChart() {
            const ctx = document.getElementById('banditDecayChart').getContext('2d');
            const alphaSlider = document.getElementById('alpha-slider');
            const alphaValueSpan = document.getElementById('alpha-value');

            function getDecayData(alpha) {
                const steps = 20;
                const stationaryData = Array.from({ length: steps }, (_, i) => 1 / (i + 1));
                const nonStationaryData = Array.from({ length: steps }, (_, i) => alpha * Math.pow(1 - alpha, i));
                return { stationaryData, nonStationaryData };
            }
            
            const initialData = getDecayData(0.1);

            banditDecayChart = new Chart(ctx, {
                type: 'line',
                data: {
                    labels: Array.from({ length: 20 }, (_, i) => `Step ${i+1}`),
                    datasets: [{
                        label: 'Stationary (Hyperbolic)',
                        data: initialData.stationaryData,
                        borderColor: '#3498db',
                        backgroundColor: 'rgba(52, 152, 219, 0.1)',
                        fill: false,
                        tension: 0.1
                    }, {
                        label: 'Non-Stationary (Exponential)',
                        data: initialData.nonStationaryData,
                        borderColor: '#e67e22',
                        backgroundColor: 'rgba(230, 126, 34, 0.1)',
                        fill: false,
                        tension: 0.1
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        y: {
                            beginAtZero: true,
                            title: { display: true, text: 'Weight of Reward' }
                        },
                        x: {
                             title: { display: true, text: 'Time Steps Ago' }
                        }
                    },
                    plugins: {
                        title: {
                            display: true,
                            text: 'Reward Weight Decay Over Time'
                        }
                    }
                }
            });

            alphaSlider.addEventListener('input', (e) => {
                const alpha = parseFloat(e.target.value);
                alphaValueSpan.textContent = alpha.toFixed(2);
                const newData = getDecayData(alpha);
                banditDecayChart.data.datasets[1].data = newData.nonStationaryData;
                banditDecayChart.update();
            });
        }

        function initComparator() {
            const sel1 = document.getElementById('comparator-1');
            const sel2 = document.getElementById('comparator-2');
            const col1 = document.getElementById('comparator-col-1');
            const col2 = document.getElementById('comparator-col-2');

            function updateComparator() {
                const val1 = sel1.value;
                const val2 = sel2.value;
                
                col1.innerHTML = renderComparison(appData.comparisons[val1]);
                col2.innerHTML = renderComparison(appData.comparisons[val2]);
            }

            function renderComparison(data) {
                let propertiesHtml = data.properties.map(p => `<li class="text-gray-600">${p.replace(/\$(.*?)\$/g, '<span class="font-mono">$1</span>')}</li>`).join('');
                return `
                    <h4 class="text-lg font-bold text-gray-800">${data.title}</h4>
                    <p class="mt-1 text-sm text-gray-500">${data.description}</p>
                    <ul class="mt-4 list-disc list-inside space-y-1 text-sm">${propertiesHtml}</ul>
                    <div class="mt-4 bg-red-50 border-l-4 border-red-400 p-3 rounded-r-lg">
                        <p class="text-sm text-red-700"><strong>Weakness:</strong> ${data.cons}</p>
                    </div>
                `;
            }

            sel1.addEventListener('change', updateComparator);
            sel2.addEventListener('change', updateComparator);
            updateComparator();
        }

        function initCliffWorld() {
            const grid = document.getElementById('cliff-grid');
            const sarsaBtn = document.getElementById('sarsa-btn');
            const qlearningBtn = document.getElementById('qlearning-btn');
            const explanation = document.getElementById('cliff-explanation');
            const nRows = 4;
            const nCols = 12;

            function drawGrid() {
                 grid.innerHTML = '';
                 for (let i = 0; i < nRows * nCols; i++) {
                    const cell = document.createElement('div');
                    cell.classList.add('cliff-grid-cell', 'border', 'border-gray-300', 'bg-white');
                    // Set the ID directly on the cell BEFORE adding content or accessing its parent
                    const row = Math.floor(i / nCols);
                    const col = i % nCols;
                    cell.id = `cell-${row}-${col}`;

                    const content = document.createElement('div');
                    content.classList.add('cliff-grid-cell-content', 'flex', 'items-center', 'justify-center', 'font-bold', 'text-gray-700');

                    if (row === nRows - 1 && col > 0 && col < nCols - 1) {
                         content.textContent = 'C';
                         // Add class to cell instead of content.parentElement
                         cell.classList.add('bg-gray-800', 'text-white');
                    } 

                    if (row === nRows - 1 && col === 0) content.textContent = 'S';
                    if (row === nRows - 1 && col === nCols - 1) content.textContent = 'G';
                    
                    cell.appendChild(content);
                    grid.appendChild(cell);
                }
            }

            function clearPaths() {
                const cells = grid.querySelectorAll('.cliff-grid-cell');
                cells.forEach(cell => {
                    cell.classList.remove('bg-blue-300', 'bg-red-300');
                    const row = parseInt(cell.id.split('-')[1]);
                    const col = parseInt(cell.id.split('-')[2]);
                    if (row === 3 && col > 0 && col < 11) {
                        cell.classList.add('bg-gray-800');
                    } else {
                        cell.classList.remove('bg-gray-800');
                    }
                });
            }

            sarsaBtn.addEventListener('click', () => {
                clearPaths();
                explanation.textContent = appData.cliffWorld.sarsa;
                explanation.classList.remove('text-red-700');
                explanation.classList.add('text-blue-700');
                // Path: (3,0) -> (2,0) -> (2,1) ... (2,11) -> (3,11)
                for(let i=0; i <= 11; i++) {
                    document.getElementById(`cell-2-${i}`).classList.add('bg-blue-300');
                }
                 document.getElementById(`cell-3-0`).classList.add('bg-blue-300');
                 document.getElementById(`cell-3-11`).classList.add('bg-blue-300');
            });
            
            qlearningBtn.addEventListener('click', () => {
                clearPaths();
                explanation.textContent = appData.cliffWorld.qlearning;
                explanation.classList.remove('text-blue-700');
                explanation.classList.add('text-red-700');
                // Path: (3,0) -> (3,1) -> ... -> (3,11)
                for(let i=0; i <= 11; i++) {
                     document.getElementById(`cell-3-${i}`).classList.add('bg-red-300');
                }
            });

            drawGrid();
        }

        let hrlOptimalityChart;
        function initHRLChart() {
             const ctx = document.getElementById('hrlOptimalityChart').getContext('2d');
             hrlOptimalityChart = new Chart(ctx, {
                type: 'bar',
                data: {
                    labels: ['Recursively Optimal (R)', 'Hierarchically Optimal (H)', 'Flat Optimal (F)'],
                    datasets: [{
                        label: 'Expected Return',
                        data: [80, 95, 100], 
                        backgroundColor: [
                            'rgba(255, 159, 64, 0.5)',
                            'rgba(54, 162, 235, 0.5)',
                            'rgba(75, 192, 192, 0.5)'
                        ],
                        borderColor: [
                             'rgba(255, 159, 64, 1)',
                            'rgba(54, 162, 235, 1)',
                            'rgba(75, 192, 192, 1)'
                        ],
                        borderWidth: 1
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    indexAxis: 'y',
                    scales: {
                        x: {
                            beginAtZero: true,
                            title: { display: true, text: 'Relative Expected Reward' }
                        }
                    },
                     plugins: {
                        title: {
                            display: true,
                            text: 'HRL Optimality Relationship (R ≤ H ≤ F)'
                        },
                        legend: { display: false },
                        tooltip: {
                            callbacks: {
                                label: function(context) {
                                    const label = context.label || '';
                                    if (label.includes('Recursive')) {
                                        return 'Optimal for each sub-problem independently.';
                                    }
                                    if (label.includes('Hierarchical')) {
                                        return 'Optimal within the constraints of the hierarchy.';
                                    }
                                    if (label.includes('Flat')) {
                                        return 'The true global optimum without hierarchical constraints.';
                                    }
                                    return '';
                                }
                            }
                        }
                    }
                }
             });
        }
        
        function initQuizzes() {
            Object.keys(appData.quizzes).forEach(quizId => {
                const container = document.getElementById(`${quizId}-quiz`);
                if(container) {
                    const quizData = appData.quizzes[quizId];
                    let currentQuestionIndex = 0;
                    container.innerHTML = `
                        <div class="bg-white p-6 rounded-lg shadow-lg">
                            <h3 class="text-2xl font-bold mb-4 text-center">Test Your Knowledge</h3>
                            <div id="${quizId}-question-area"></div>
                            <div class="text-center mt-6">
                                <button id="${quizId}-next-btn" class="bg-gray-600 text-white font-bold py-2 px-6 rounded-lg hover:bg-gray-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-gray-500 hidden">Next Question</button>
                            </div>
                        </div>
                    `;
                    
                    const questionArea = document.getElementById(`${quizId}-question-area`);
                    const nextBtn = document.getElementById(`${quizId}-next-btn`);

                    function loadQuestion(index) {
                        nextBtn.classList.add('hidden');
                        const q = quizData[index];
                        const optionsHtml = q.options.map((opt, i) => 
                            `<button data-index="${i}" class="quiz-option block w-full text-left p-3 mt-2 border border-gray-300 rounded-lg hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-blue-500">${opt}</button>`
                        ).join('');

                        questionArea.innerHTML = `
                            <p class="text-lg font-medium text-gray-800">${index + 1}. ${q.question}</p>
                            <div class="mt-4">${optionsHtml}</div>
                            <div id="${quizId}-solution-${index}" class="mt-4 hidden"></div>
                        `;

                        questionArea.querySelectorAll('.quiz-option').forEach(btn => {
                            btn.addEventListener('click', () => checkAnswer(index, parseInt(btn.dataset.index)));
                        });
                    }

                    function checkAnswer(qIndex, selectedIndex) {
                        const q = quizData[qIndex];
                        const options = questionArea.querySelectorAll('.quiz-option');
                        options.forEach((btn, i) => {
                            btn.disabled = true;
                            if (i === q.correct) {
                                btn.classList.add('bg-green-200', 'border-green-400');
                            }
                            if (i === selectedIndex && i !== q.correct) {
                                btn.classList.add('bg-red-200', 'border-red-400');
                            }
                        });

                        const solutionDiv = document.getElementById(`${quizId}-solution-${qIndex}`);
                        solutionDiv.innerHTML = `
                             <div class="bg-blue-50 border-l-4 border-blue-400 p-4 rounded-r-lg">
                                <h4 class="font-bold text-blue-800">Explanation:</h4>
                                <p class="text-sm text-blue-700">${q.explanation}</p>
                             </div>
                        `;
                        solutionDiv.classList.remove('hidden');
                        if (currentQuestionIndex < quizData.length - 1) {
                            nextBtn.classList.remove('hidden');
                        }
                    }

                    nextBtn.addEventListener('click', () => {
                        currentQuestionIndex++;
                        loadQuestion(currentQuestionIndex);
                    });

                    loadQuestion(currentQuestionIndex);
                }
            });
        }

    </script>
</body>
</html>
